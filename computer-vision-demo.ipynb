{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References and Resources\n",
        "- This tuto is inspired by : https://github.com/azinonos/MNIST_DL_Tutorial\n",
        "\n",
        "- Neural Network structure Visualization : https://alexlenail.me/NN-SVG/index.html\n",
        "\n",
        "- All cell outputs in orange color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import necessary Deep Learning librairies (keras, sklearn...) and set initial parameters\n",
        "- Number of layers and depth can be adapted (default : 2 x 128 neuron layers)\n",
        "\n",
        "- batch_size and epochs can be also set with variables here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "ZnTkiVfzRuVc",
        "outputId": "55862d93-08f0-492f-f586-32af0c52427d"
      },
      "outputs": [],
      "source": [
        "#########################################################################################################################################\n",
        "# Computer Vision Demo Jupyter Notebook based on MNSIT dataset\n",
        "# Last Updated: 2024 08 27\n",
        "#########################################################################################################################################\n",
        "#\n",
        "# Use a python3 virtualenv with Python >= 3.10.x (venv or anaconda)\n",
        "#\n",
        "# With conda use :\n",
        "#    % conda create --name computer-vision-demo python=3.10\n",
        "#    % python --version\n",
        "#      Python 3.10.14\n",
        "#    % conda env list\n",
        "#    % conda activate computer-vision-demo\n",
        "#    % conda install --file requirements.txt (or pip3 install -r requirements.txt)\n",
        "#\n",
        "# Recommandations : use numpy<2.x with Tensorflow to avoid conflicts\n",
        "\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Install keras in anaconda : https://oindrilasen.com/2021/02/how-to-install-and-import-keras-in-anaconda-jupyter-notebooks/\n",
        "#   % conda install anaconda::keras\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Input, Dense, Dropout\n",
        "from keras.backend import clear_session\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "import importlib\n",
        "\n",
        "%matplotlib inline\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # suppress tensorflow warnings\n",
        "\n",
        "# Debug : Measure time to import modules\n",
        "# modules = ['os', 'time', 'numpy', 'pandas', 'matplotlib.pyplot', 'keras', 'sklearn.metrics', 'seaborn', 'IPython.display']\n",
        "# for module in modules:\n",
        "#    start_time = time.time()\n",
        "#    importlib.import_module(module)\n",
        "#   end_time = time.time()\n",
        "#    print(f\"Importing {module} took {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "# Package used to plot the training phase\n",
        "# See also : https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b\n",
        "from livelossplot import PlotLossesKeras\n",
        "\n",
        "# Hyper-parameters that define the number and depth of hidden layers\n",
        "_hidden_layer_number=2       # Number of layers in the Neural Network (min=2)\n",
        "_hidden_layer_depth=128      # Depth of each layer = number of Neurons\n",
        "_training_batch_size=1024    # Define the batch size for training (number of samples to process in parallel)\n",
        "_training_epochs=10          # Define the number of epochs for training (iterations over the entire dataset)\n",
        "_random_seed = 7             # Default seed to initialze the numpy random number generator (rng)\n",
        "_output_cell_color='#FF5E26' # Official Pure Storage Orange for outputs\n",
        "\n",
        "# A crucial step in ensuring reproducibility is the Numpy Librairie initialization !\n",
        "#\n",
        "# np.random.seed(0) sets the seed for the NumPy random number generator to a fixed value (in this case, 0). \n",
        "#\n",
        "# This ensures that the subsequent random number generations will be deterministic and reproducible.\n",
        "# When you set a seed for the random number generator, it means that the sequence of random numbers generated \n",
        "# by the algorithm will always start with the same value. \n",
        "# This is useful when:\n",
        "#    - You want to reproduce results: By setting the seed, you can guarantee that your code will produce the \n",
        "#      same output every time, which is essential for reproducing experimental results or sharing code with others.\n",
        "#    - You need consistent testing: Randomness can be difficult to test, but by fixing the seed, you can create a \n",
        "#      controlled environment for testing and validation.\n",
        "# In this specific case, setting the seed might not have a direct impact on your MNIST dataset analysis, \n",
        "# but it's still a good practice to ensure reproducibility and consistency in your code.\n",
        "\n",
        "### DEPRECATED NUMPY RANDOM NUMBER GENERATOR CODE => Old style\n",
        "# start_time = time.time()\n",
        "# np.random.seed(0)\n",
        "# elapsed_time = (time.time() - start_time)\n",
        "# display(HTML(f\"<h1 style='color:{_output_cell_color};'>Neural Network has been initialized with random seed: \\\n",
        "#         {_random_seed} in {elapsed_time:.2f} seconds</h1>\"))\n",
        "\n",
        "### NEW NUMPY RANDOM NUMBER GENERATOR CODE => New recommended Random Generator\n",
        "start_time = time.time()\n",
        "\n",
        "# Create a SeedSequence instance using our initial _random_seed\n",
        "seed_sequence = np.random.SeedSequence(_random_seed)\n",
        "\n",
        "# Create a BitGenerator instance using the SeedSequence\n",
        "bit_generator = np.random.PCG64(seed_sequence)\n",
        "\n",
        "# Create a Generator instance (: rng will be used in our code to generate random numbers)\n",
        "rng = np.random.Generator(bit_generator)\n",
        "\n",
        "# Use the Generator instance for random number generation (test example)\n",
        "#    random_numbers = rng.random(10)  # Generates 10 random numbers\n",
        "#    print(random_numbers)\n",
        "\n",
        "elapsed_time = (time.time() - start_time)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Neural Network has been initialized with random seed: \\\n",
        "             {_random_seed} in {elapsed_time:.2f} seconds</h1>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "itkJl2JhSV3D"
      },
      "source": [
        "# Split the Dataset to prepare for training and testing phases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "-COGMFL7SMcr",
        "outputId": "45dc2db5-14e4-4eb5-fdf8-00a05d119670"
      },
      "outputs": [],
      "source": [
        "# Import the MNIST Dataset directly from keras included datasets\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Print the shape of the training and tests sets (images + labels)\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The \\'x_train\\' structure shape (images) is {x_train.shape}</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The \\'y_train\\' structure shape (labels) is {y_train.shape}</h1>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3MN8ehUcSqqz"
      },
      "source": [
        "# Visualize all the possible output classes (or modalities) images from the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "colab_type": "code",
        "id": "n2psUUkISllz",
        "outputId": "f3454570-1441-49fb-bf3e-c0a0136ace23"
      },
      "outputs": [],
      "source": [
        "# Define the number of classes (e.g., digits 0-9)\n",
        "num_classes = 10\n",
        "\n",
        "# Create a figure with subplots for each class\n",
        "f, ax = plt.subplots(1, num_classes, figsize=(20,20))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display images of all possible \\'modalities\\' in \\\n",
        "             the MNIST dataset (total of 10 modalities): </h1>\"))\n",
        "\n",
        "# Loop on the x_train dataset to display all possible modalities in the dataset (test on y_test that we know is true)\n",
        "for i in range(0, num_classes):\n",
        "  # Get the first sample from the current class (assuming x_train and y_train are available)\n",
        "  sample = x_train[y_train == i][0]\n",
        "\n",
        "  # Display the sample as an image using grayscale colormap\n",
        "  ax[i].imshow(sample, cmap='gray')\n",
        "\n",
        "  # Set the title of the subplot with the label number\n",
        "  ax[i].set_title(\"Label: {}\".format(i), fontsize=16)\n",
        "\n",
        "  # Remove the axis for better visualization\n",
        "  ax[i].axis('off')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize a random sample image from the training set in its numeric matrix form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a random index and print the [28x28] matrix corresponding to the image at this index\n",
        "# We will set 0 and 1 values only in the matrix to show the numbers with '1'\n",
        "random_idx = np.random.choice(len(x_train))\n",
        "\n",
        "# Print image matrix with values normalized to 0 ('-') or 1 ('x')\n",
        "image = x_train[random_idx].reshape((28, 28))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display an image at a random index \\'{random_idx}\\' \\\n",
        "             in the x_train MNIST dataset'</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>This is what the Neural Network will have as an input: \\\n",
        "             can you see that it is a \\'{y_train[random_idx]}\\' ??</h1>\"))\n",
        "\n",
        "for row in image:\n",
        "  for pixel in row:\n",
        "    if pixel > 127:\n",
        "      print(str(pixel).zfill(3), end=' ') \n",
        "    else:\n",
        "      print(str(pixel).zfill(3), end=' ') \n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Now, we will transform y_train & y_test labels from integers (0 to 9) into a 'one-hot-encoded' matrix:\n",
        "\n",
        "<b>\n",
        "    For example :\n",
        "\n",
        "      [5 0 4 ... 1 3 2] \n",
        "    \n",
        "    will be transformed in this form :\n",
        "\n",
        "      [[[0. 0. 0. 0. 0. 1. ... 0. 0. 0.]  ===> one-hot encodeing for 5\n",
        "        [1. 0. 0. 0. 0. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 0\n",
        "        [0. 0. 0. 0. 1. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 4\n",
        "        ...\n",
        "        [0. 1. 0. 0. 0. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 1\n",
        "        [0. 0. 0. 1. 0. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 3\n",
        "        [0. 0. 1. 0. 0. 0. ... 0. 0. 0.]] ===> one-hot encodeing for 2\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In Keras, when working with categorical labels (i.e., discrete outcomes), you need to convert them into a categorical format. \n",
        "# This is because the default loss function and optimization algorithms assume that the outputs are real-valued (continuous).\n",
        "\n",
        "# The to_categorical() function from Keras' utils module does just that: it converts integer-encoded categorical labels into one-hot \n",
        "# encoded arrays.\n",
        "#\n",
        "# Here's what happens:\n",
        "#    For each sample in y_train and y_test, you get a vector of length num_classes, where:\n",
        "#       The index corresponding to the true class label is set to 1 (i.e., True or 1).@\n",
        "#       All other indices are set to 0 (i.e., False or 0).\n",
        "# This one-hot encoded representation allows you to use categorical cross-entropy loss functions, such as categorical_crossentropy, \n",
        "# which is commonly used for multi-class classification problems.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Now, \\'y_train\\' labels has been \\'hot-encoded\\' and look like below : \\\n",
        "             the correpsonding label is where you see a \\'1\\'.</h1>\"))\n",
        "\n",
        "# Loop through some samples in the training data and print its label again (after conversion)\n",
        "for i in range(4):\n",
        "  print(y_train[i])\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>This is necessary to use the \\'categorical cross-entropy loss function\\'</h1>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uwQcpaBuUTe_"
      },
      "source": [
        "# Prepare Data for the training phase\n",
        "\n",
        "### Step 1 : Normalize input images in theh range [0,1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yN6M-is5T0nu"
      },
      "outputs": [],
      "source": [
        "# Normalize the pixel values of the training images by dividing each value by 255.0\n",
        "\n",
        "# This code is normalizing the pixel values of the training and test images to be between 0 and 1, \n",
        "# which is a common practice in deep learning models.\n",
        "# In this case, it's likely that the original pixel values were between 0 and 255 (i.e., 8-bit grayscale), \n",
        "# so dividing each value by 255.0 brings them down to the range [0, 1]. This normalization helps to:\n",
        "#   Reduce the effect of large pixel values on the model's training and testing\n",
        "#   Make the model more robust to small variations in brightness or contrast\n",
        "#   Improve the convergence and accuracy of the model\n",
        "# Note that this is a simple form of data normalization. \n",
        "# In some cases, you may need to use more sophisticated techniques, such as mean subtraction or standardization, \n",
        "# depending on the specific characteristics of your dataset.\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Pick a random image from the x_train set with a random index\n",
        "# random_idx = np.random.choice(len(x_train))     # Deprecated random numpy generator \n",
        "random_idx = rng.choice(len(x_train))\n",
        "\n",
        "image = x_train[random_idx].reshape((28, 28))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>We picked a random image at index \\'{random_idx}\\' \\\n",
        "             in the x_train MNIST dataset.</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The image matrix from \\'x_train\\'x transformed with \\\n",
        "             decimal values .xxxx is displayed below.</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The corresponding \\'y_train\\'x encoded label is: \\' \\\n",
        "            {y_train[random_idx]}\\'</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>===> Can you say which number is it ??</h1>\"))\n",
        "\n",
        "# Print the image matrix with values in the range [0,1]\n",
        "for row in image:\n",
        "    for pixel in row:\n",
        "        if pixel > 127:\n",
        "            print(\"{:.4f}\".format(pixel), end=' ')  # Display pixel value with 3 decimal places\n",
        "        else:\n",
        "            print(\"{:.4f}\".format(pixel), end=' ')  # Display pixel value with 3 decimal places\n",
        "    print()\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>...It is a {np.where(y_train[random_idx] ==1)[0][0]} !</h1>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Data for the training phase\n",
        "\n",
        "### Step 2 : Reshape images from  (height, width, channels) to (samples, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "x2kNxq2rUsoG",
        "outputId": "ddf2b84e-130c-42ac-a720-1b638d270c1d"
      },
      "outputs": [],
      "source": [
        "# Reshape the test images from (height, width, channels) to (samples, features)\n",
        "\n",
        "# The -1 in the reshape method is a special value that tells NumPy to automatically calculate \n",
        "# the size based on the other dimensions. \n",
        "# In this case, it means that the first dimension (the number of samples) will remain unchanged, \n",
        "# and the second dimension (the number of features) will be calculated as the product of the height, \n",
        "# width, and channels.\n",
        "\n",
        "# Before reshaping, x_train likely had a shape like ( Samples, Height, Width, Channels ), \n",
        "# where each image is represented by a 4D array with three dimensions for spatial information \n",
        "# (height, width, and channels).\n",
        "\n",
        "# After reshaping, x_train will have a shape like (Samples, Features), where the number of features \n",
        "# is equal to the product of height, width, and channels. \n",
        "# This is often referred to as a \"flat\" or \"vectorized\" representation, which is suitable for \n",
        "# inputting into neural networks.\n",
        "\n",
        "# By printing the shape of x_train using print(x_train.shape), you can verify that the reshaping was \n",
        "# successful and check the new shape of your data.\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>We reshaped the input \\'x_train\\' images to \\\n",
        "             be flat with the 28x28 pixels of each images.</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>\\'x_train\\' shape is now: {x_train.shape} so \\\n",
        "             we have 60000 images of 784 pixels each</h1>\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9-K1km4JVQpe"
      },
      "source": [
        "# Create the Model : a Fully Fast Forward Connected Neural Network (CNN\n",
        "- Step 1: Create the input flaten layer (768 pixels = 28 x 28)\n",
        "\n",
        "- Step 2 : Create one or several hidden layers (adjustable depths with variable '_hidden_layer_depth')\n",
        "\n",
        "- Step 3 : Create the output layer : here we have 10 modalities (or classes), so 10 output neurons for the classifier\n",
        "\n",
        "- Step 4 : print the NN layout and total numbers of parameters (it can easyly reach 1 Millions parametres !!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "id": "pOE4hD2HUxZt",
        "outputId": "0748afca-9037-427d-80d0-3b0ba8902ee1"
      },
      "outputs": [],
      "source": [
        "# Clear the current Keras session\n",
        "clear_session()\n",
        "\n",
        "# Create a neural network model using the Sequential API\n",
        "model = Sequential()\n",
        "\n",
        "# Add a dense (fully-connected) layer with default of '_hidden_layer_depth'units, input shape (784,), \n",
        "# and ReLU activation. This layer will process the input data and produce an output\n",
        "\n",
        "# INPUT LAYER\n",
        "\n",
        "#model.add(Dense(units=_hidden_layer_depth, input_shape=(784,), activation='relu'))    # Deprecated method\n",
        "model.add(Input(shape=(784,), name='input_layer'))\n",
        "\n",
        "# HIDDEN LAYERS\n",
        "\n",
        "for i in range(_hidden_layer_number):\n",
        "    # Add another dense layer with a default of 128 units and ReLU activation\n",
        "    # This layer will process the output from the previous layer and produce another output\n",
        "    #model.add(Dense(units=_hidden_layer_depth, activation='relu')\n",
        "    model.add(Dense(units=_hidden_layer_depth, activation='relu', name=f'hidden_layer_{i+1}'))\n",
        "\n",
        "# DROPOUT LAYER\n",
        "\n",
        "# Add a dropout layer with a drop rate of 0.25 (25%)\n",
        "# Dropout randomly sets 25% of the neurons to zero during training to prevent overfitting\n",
        "model.add(Dropout(0.25, name='dropout_layer'))\n",
        "\n",
        "# OUTPUT LAYER\n",
        "\n",
        "# Add a final dense layer with 10 units and softmax activation\n",
        "# This layer will produce a probability distribution over the 10 classes\n",
        "model.add(Dense(units=10, activation='softmax', name='output_layer'))\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss function, Adam optimizer, and accuracy metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Our Neural network is composed of 1 Input layer, {_hidden_layer_number} \\\n",
        "             hidden layers with each {_hidden_layer_depth} neurons and one Output Layer as described in the summary</h1>\"))\n",
        "\n",
        "# Print a summary of the model architecture using the `summary()` method\n",
        "model.summary()\n",
        "\n",
        "#                     +---------------+\n",
        "#                     |  Input Layer  |\n",
        "#                     +---------------+\n",
        "#                             |\n",
        "#                             | (784,)\n",
        "#                             v\n",
        "# +-----------------------+       +-----------------------+\n",
        "# |      Dense Layer      |       |      Dense Layer      |\n",
        "# |      (128, ReLU)      |       |      (128, ReLU)      |\n",
        "# +-----------------------+       +-----------------------+\n",
        "#                             |\n",
        "#                             | (0.25 dropout)\n",
        "#                             v\n",
        "# +-----------------------+       +-----------------------+\n",
        "# |      Dropout Layer    |       |        None           |\n",
        "# +-----------------------+       +-----------------------+\n",
        "#                             |\n",
        "#                             |\n",
        "#                             v\n",
        "# +-----------------------+       +-----------------------+\n",
        "# |     Final Dense Layer |       |  Softmax Activation   |\n",
        "# |      (10, Softmax)    |       |                       |\n",
        "# +-----------------------+       +-----------------------+\n",
        "\n",
        "# Note that this schema is a simplified representation of the model architecture. \n",
        "# The actual connections between layers may be more complex and involve additional \n",
        "# operations such as convolutional layers, pooling layers, etc.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7bk3ylbU1Ws4"
      },
      "source": [
        "# Train the model by using tmethod 'fit' and define a bath_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "d75pU9l31S52",
        "outputId": "6ebaa4c8-fd10-4ce0-8773-56f0e1e2f621"
      },
      "outputs": [],
      "source": [
        "# Batch size for training (number of samples to process in parallel)\n",
        "#    See variable : _training_batch_size\n",
        "\n",
        "# Define the number of epochs for training (iterations over the entire dataset)\n",
        "#    See variable : _training_epochs\n",
        "\n",
        "# Full stack trace mode for debugging\n",
        "# keras.config.disable_traceback_filtering()\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "# Train the model using the `fit` method\n",
        "# x_train: input data, y_train: target labels\n",
        "model.fit(x=x_train, y=y_train, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[PlotLossesKeras()],\n",
        "          verbose = 0,\n",
        "          batch_size=_training_batch_size,\n",
        "          epochs=_training_epochs)\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Elapsed time for the training phase\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The model.fit() function to train our model \\\n",
        "             took {elapsed_time:.2f} seconds to run.</h1>\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J9Wa4rvE1qMs"
      },
      "source": [
        "# Evaluate the model by calculationg the Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "_4lONmsI1g9y",
        "outputId": "7a33457e-21c6-4299-c1af-f14743f5cff6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the 'test' dataset using the `evaluate` method\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Loss after training phase = {round(test_loss, 2)}</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Accuracy after training phase = {round(test_acc, 2)}</h1>\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use the model to make predictions on the x_test dataset (never seen by the model during training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "colab_type": "code",
        "id": "wi7yXzCI10Kr",
        "outputId": "213eccb4-ad5c-4764-f2af-f589cad1a059"
      },
      "outputs": [],
      "source": [
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Now we can predict all labels for the \\'y_test\\' testing set:</h1>\"))\n",
        "\n",
        "# Here's what each part does:\n",
        "#    model.predict(x_test) : This line uses the trained model to make predictions on the test data. \n",
        "#    The output is a NumPy array containing the predicted probability distribution for each sample\n",
        "#    in the test data.\n",
        "#\n",
        "#    y_pred_classes = np.argmax(y_pred, axis=1) : This line converts the predicted probabilities \n",
        "#    into class labels by taking the index of the maximum value along the specified axis (in this case, axis 1). \n",
        "#    The output is a NumPy array containing the predicted class labels for each sample in the test data.\n",
        "#\n",
        "#    print(y_pred) : This line prints out the predicted probability distributions for each sample in the test data. \n",
        "#    Each row represents a single prediction, and the columns represent the different classes (or outcomes) that the model can predict.\n",
        "#\n",
        "#    print(y_pred_classes) : This line prints out the predicted class labels for each sample in the test data.\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Convert the predicted probabilities into class labels using `argmax`\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>y_pred_classes table contains all predicted \\\n",
        "             labels (with a total of {len(y_pred_classes)} labels)</h1>\"))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>y_pred_classes table contains all \\\n",
        "             predicted labels : {y_pred_classes}</h1>\"))\n",
        "\n",
        "# Dataframe with predicted labels to have a better display...\n",
        "df = pd.DataFrame(y_pred_classes, columns=['Predicted Classes'])\n",
        "df = df.reset_index(drop=True).reset_index().rename(columns={'index': 'y_pred Index'})\n",
        "\n",
        "# Display the 100 fisrt predicted labels\n",
        "print(df.head(20).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pick a random image form x_test the run the prediction and display results with 'predict' and 'true'\n",
        "\n",
        "Note : each run of this cell will pick a different image form the x_test dataset (random index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "colab_type": "code",
        "id": "UxaVbpGo2ZOm",
        "outputId": "fa584997-bf0b-4524-bfa9-26242a01d4c6"
      },
      "outputs": [],
      "source": [
        "# Single PREDICTION Example on a random sample from the x_test dataset \n",
        "# (not known during the training phase)\n",
        "\n",
        "# Select a single random example from the test data for visualization by choosing a \n",
        "# random index within the length of x_test\n",
        "\n",
        "#random_idx = np.random.choice(len(x_test))\n",
        "random_idx = rng.choice(len(x_test))\n",
        "\n",
        "# Get the input data (features) at the chosen index\n",
        "x_sample = x_test[random_idx]\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Predicting Image at Index = {random_idx}, \\\n",
        "             Size of x_sample = {len(x_sample)}</h1>\"))\n",
        "\n",
        "# Now, extract the true and predicted class labels for this example\n",
        "\n",
        "# And now, we have to onvert the one-hot encoded label to a class label (0-9)\n",
        "\n",
        "# Let's explain more in depth this part:\n",
        "#   np.argmax: This is a function from the NumPy library that finds the index of the maximum value \n",
        "#      along a specified axis.\n",
        "#\n",
        "#   y_test: This is a 2D NumPy array representing the true labels for the test data. Each row corresponds \n",
        "#      to a single sample, and each column represents a possible class label (in this case, digit classes 0-9).\n",
        "#\n",
        "#   axis=1: This specifies that we want to find the maximum value along the second axis (i.e., the rows). \n",
        "#     In other words, we're looking for the most likely class label for each individual sample.\n",
        "#\n",
        "# When you run this code, NumPy will iterate through each row of y_test, find the maximum value in that row, \n",
        "# and return the corresponding index. \n",
        "#\n",
        "# => This is equivalent to converting the one-hot encoded labels into a single class label (e.g., [0, 0, 1, 0] \n",
        "# becomes 2, since 2 is the most likely class).\n",
        "# The resulting array y_true will contain the true class labels for each sample in the test data.\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get the true class label for this example\n",
        "y_sample_true = y_true[random_idx]\n",
        "\n",
        "# Get the predicted class label for this example\n",
        "y_sample_pred_class = y_pred_classes[random_idx]\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>\\'Predicted Label\\' versus \\'True label\\' in y_test</h1>\"))\n",
        "\n",
        "plt.title(\"Predicted: {}, True: {}\".format(y_sample_pred_class, y_sample_true), fontsize=16)\n",
        "plt.imshow(x_sample.reshape(28, 28), cmap='gray')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q2kYQNGJ3aN5"
      },
      "source": [
        "# Calculate and display the Confusion Matrix of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "colab_type": "code",
        "id": "77SFgjZ53KJ4",
        "outputId": "dde148b9-f8dd-4b54-cf6b-464c0e32c835"
      },
      "outputs": [],
      "source": [
        "# This code calculates the confusion matrix : a table that summarizes the predictions made by a \n",
        "# classification model, using the true and predicted labels. \n",
        "# Then, it creates a plot for the confusion matrix using Seaborn's heatmap function. \n",
        "# The annot=True argument means that each cell in the heatmap will be annotated with the number \n",
        "# of correct/incorrect predictions. \n",
        "# The fmt='d' argument specifies the format of the annotations (in this case, decimal integers).\n",
        "\n",
        "# Calculate the confusion matrix using true and predicted labels\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display the Confusion Matrix of our model \\\n",
        "             showing \\'Predicted\\' versus \\'True\\'</h1>\"))\n",
        "\n",
        "# Create a plot for the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "ax = sns.heatmap(confusion_mtx, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
        "ax.set_xlabel('Predicted Label')\n",
        "ax.set_ylabel('True Label')\n",
        "ax.set_title('Confusion Matrix');\n",
        "\n",
        "# Display the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n-9WrS6w4TO0"
      },
      "source": [
        "# Investigate Errors of our Model, in which case it fails do predict correctly ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "colab_type": "code",
        "id": "99OZQyuI5H6h",
        "outputId": "95258142-08a0-42c4-db42-280d86f844fa"
      },
      "outputs": [],
      "source": [
        "# This code identifies the incorrect predictions by subtracting the true labels from \n",
        "# the predicted class labels. \n",
        "# It then extracts the relevant information (predicted classes, probabilities, true labels, \n",
        "# and input data) only for these incorrect predictions.\n",
        "\n",
        "# The errors array is a boolean mask that indicates which indices in the original arrays correspond \n",
        "# to incorrect predictions.\n",
        "# The indexing operation [errors] uses this mask to extract the corresponding elements from each array, \n",
        "# resulting in separate arrays for the errors (incorrect predictions).\n",
        "\n",
        "# Create a boolean array indicating where predictions are incorrect\n",
        "errors = (y_pred_classes - y_true != 0)\n",
        "\n",
        "# Extract the predicted classes, predicted probabilities, true labels, and \n",
        "# input data for incorrect predictions only\n",
        "y_pred_classes_errors = y_pred_classes[errors]  # Predicted class labels for errors\n",
        "y_pred_errors = y_pred[errors]                  # Predicted probabilities for errors\n",
        "y_true_errors = y_true[errors]                  # True class labels for errors\n",
        "x_test_errors = x_test[errors]                  # Input data (features) for errors\n",
        "\n",
        "# Calculate the maximum predicted error probability for each sample\n",
        "y_pred_errors_probability = np.max(y_pred_errors, axis=1)\n",
        "\n",
        "# Extract the true class probability errors from the predicted errors\n",
        "true_probability_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n",
        "\n",
        "# Compute the difference between the predicted error probability and the true class probability error\n",
        "diff_errors_pred_true = y_pred_errors_probability - true_probability_errors\n",
        "\n",
        "# Get list of indices of sorted differences\n",
        "sorted_idx_diff_errors = np.argsort(diff_errors_pred_true)\n",
        "top_idx_diff_errors = sorted_idx_diff_errors[-5:] # 5 last ones\n",
        "\n",
        "# Show Top Errors\n",
        "num = len(top_idx_diff_errors)\n",
        "f, ax = plt.subplots(1, num, figsize=(30,30))\n",
        "\n",
        "print()\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display samples where our model is failing the most \\\n",
        "             (and compare with the confusion matrix above)</h1>\"))\n",
        "\n",
        "for i in range(0, num):\n",
        "  idx = top_idx_diff_errors[i]\n",
        "  sample = x_test_errors[idx].reshape(28,28)\n",
        "  y_t = y_true_errors[idx]\n",
        "  y_p = y_pred_classes_errors[idx]\n",
        "  ax[i].imshow(sample, cmap='gray')\n",
        "  ax[i].set_title(\"Predicted label :{}\\nTrue label: {}\".format(y_p, y_t), fontsize=22)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congratulations you reached the dnd of the simple Computer Vision Demo ! \n",
        "\n",
        "## Summary of what you did :\n",
        "\n",
        "### - Import the clean MNIST dataset from keras\n",
        "### - Split the dataset with a training set and a testing set\n",
        "### - Visualize the dataset \n",
        "### - Prepare the dataset for the training phase\n",
        "### - Build a simple Neural Network model from scratch\n",
        "### - Train the model\n",
        "### - Evaluate the model (accuracy and confusion matrix)\n",
        "### - Investigate errors of the model\n",
        "\n",
        "# Thanks !"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "MNIST Digits Classfication.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
