{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# References and Resources\n",
        "- This tuto is inspired by : https://github.com/azinonos/MNIST_DL_Tutorial\n",
        "\n",
        "- Neural Network structure Visualization : https://alexlenail.me/NN-SVG/index.html\n",
        "\n",
        "- All cell outputs in orange color"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Import necessary Deep Learning librairies (keras, sklearn...) and set initial parameters\n",
        "- Number of layers and depth can be adapted (default : 2 x 128 neuron layers)\n",
        "\n",
        "- batch_size and epochs can be also set with variables here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "colab_type": "code",
        "id": "ZnTkiVfzRuVc",
        "outputId": "55862d93-08f0-492f-f586-32af0c52427d"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' # suppress tensorflow warnings\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "from keras.utils import plot_model\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Package used to plot the training phase\n",
        "# See also : https://medium.com/inbrowserai/simple-diagrams-of-convoluted-neural-networks-39c097d2925b\n",
        "from livelossplot import PlotLossesKeras\n",
        "\n",
        "# Hyper-parameters that define the number and depth of hidden layers\n",
        "_hidden_layer_number=2       # Number of layers in the Neural Network (min=2)\n",
        "_hidden_layer_depth=128      # Depth of each layer = number of Neurons\n",
        "_training_batch_size=1024    # Define the batch size for training (number of samples to process in parallel)\n",
        "_training_epochs=10          # Define the number of epochs for training (iterations over the entire dataset)\n",
        "_random_seed = 0\n",
        "_output_cell_color='#FF5E26' # Official Pure Storage Orange for outputs\n",
        "\n",
        "# A crucial step in ensuring reproducibility!\n",
        "#\n",
        "# np.random.seed(0) sets the seed for the NumPy random number generator to a fixed value (in this case, 0). \n",
        "#\n",
        "# This ensures that the subsequent random number generations will be deterministic and reproducible.\n",
        "# When you set a seed for the random number generator, it means that the sequence of random numbers generated by the algorithm will always start with the same value. \n",
        "# This is useful when:\n",
        "#    - You want to reproduce results: By setting the seed, you can guarantee that your code will produce the same output every time, which is essential for reproducing \n",
        "#      experimental results or sharing code with others.\n",
        "#    - You need consistent testing: Randomness can be difficult to test, but by fixing the seed, you can create a controlled environment for testing and validation.\n",
        "# In this specific case, setting the seed might not have a direct impact on your MNIST dataset analysis, but it's still a good practice to ensure reproducibility \n",
        "# and consistency in your code.\n",
        "start_time = time.time()\n",
        "np.random.seed(_random_seed)\n",
        "print(\"Time taken:\", time.time() - start_time)\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Neural Network has been initialized with random seed: {_random_seed}</h1>\"))\n",
        "#print(\"Neural Network has been initialized with random seed:\", _random_seed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "itkJl2JhSV3D"
      },
      "source": [
        "# Split the Dataset to prepare for training and testing phases"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "-COGMFL7SMcr",
        "outputId": "45dc2db5-14e4-4eb5-fdf8-00a05d119670"
      },
      "outputs": [],
      "source": [
        "# Import the MNIST Dataset directly from keras includzed datasets\n",
        "from keras.datasets import mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Print the shape of the training and tests sets (images + labels)\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The \\'x_train\\' structure shape (images) is {x_train.shape}</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The \\'y_train\\' structure shape (labels) is {y_train.shape}</h1>\"))\n",
        "\n",
        "#print(f'\\nx_Train shape = {x_train.shape} \\ny_train Shape = {y_train.shape}')\n",
        "#print(f'x_test shape = {x_test.shape} \\ny_test shape =  {y_test.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3MN8ehUcSqqz"
      },
      "source": [
        "# Visualize all the possible output classes (or modalities) images from the MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "colab_type": "code",
        "id": "n2psUUkISllz",
        "outputId": "f3454570-1441-49fb-bf3e-c0a0136ace23"
      },
      "outputs": [],
      "source": [
        "# Define the number of classes (e.g., digits 0-9)\n",
        "num_classes = 10\n",
        "\n",
        "# Create a figure with subplots for each class\n",
        "f, ax = plt.subplots(1, num_classes, figsize=(20,20))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display images of all possible \\'modalities\\' in the MNIST dataset (total of 10 modalities): </h1>\"))\n",
        "#print('\\nDisplay images of all possible modalities in the MNIST dataset:\\n')\n",
        "\n",
        "# Loop on the x_train dataset to display all possible modalities in the dataset (test on y_test that we know is true)\n",
        "for i in range(0, num_classes):\n",
        "  # Get the first sample from the current class (assuming x_train and y_train are available)\n",
        "  sample = x_train[y_train == i][0]\n",
        "\n",
        "  # Display the sample as an image using grayscale colormap\n",
        "  ax[i].imshow(sample, cmap='gray')\n",
        "\n",
        "  # Set the title of the subplot with the label number\n",
        "  ax[i].set_title(\"Label: {}\".format(i), fontsize=16)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Visualize a random sample image from the training set in its numeric matrix form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pick a random index and print the [28x28] matrix corresponding to the image at this index\n",
        "# We will set 0 and 1 values only in the matrix to show the numbers with '1'\n",
        "random_idx = np.random.choice(len(x_train))\n",
        "\n",
        "# Print image matrix with real values\n",
        "#print(x_train[random_idx])\n",
        "\n",
        "# Print image matrix with values normalized to 0 ('-') or 1 ('x')\n",
        "image = x_train[random_idx].reshape((28, 28))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display an image at a random index \\'{random_idx}\\' in the x_train MNIST dataset'</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>This is what the Neural Network will have as an input: can you see that it is a \\'{y_train[random_idx]}\\' ??</h1>\"))\n",
        "\n",
        "#print(\"\\nRandom image at index\", random_idx, \":\")\n",
        "\n",
        "for row in image:\n",
        "  for pixel in row:\n",
        "    if pixel > 127:\n",
        "      #print(\"O\", end=' ')\n",
        "      #print(\"{:.03d}\".format(pixel), end=' ')  # Output: 3.1\n",
        "      print(str(pixel).zfill(3), end=' ') \n",
        "    else:\n",
        "      #print(\"-\", end=' ')\n",
        "      #print(\"{:.03d}\".format(pixel), end=' ')  # Output: 3.1\n",
        "      print(str(pixel).zfill(3), end=' ') \n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Now, we will transform y_train & y_test labels from integers (0 to 9) into a 'one-hot-encoded' matrix:\n",
        "\n",
        "<b>\n",
        "    For example :\n",
        "\n",
        "      [5 0 4 ... 1 3 2] \n",
        "    \n",
        "    will be transformed in this form :\n",
        "\n",
        "      [[[0. 0. 0. 0. 0. 1. ... 0. 0. 0.]  ===> one-hot encodeing for 5\n",
        "        [1. 0. 0. 0. 0. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 0\n",
        "        [0. 0. 0. 0. 1. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 4\n",
        "        ...\n",
        "        [0. 1. 0. 0. 0. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 1\n",
        "        [0. 0. 0. 1. 0. 0. ... 0. 0. 0.]  ===> one-hot encodeing for 3\n",
        "        [0. 0. 1. 0. 0. 0. ... 0. 0. 0.]] ===> one-hot encodeing for 2\n",
        "        ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# In Keras, when working with categorical labels (i.e., discrete outcomes), you need to convert them into a categorical format. \n",
        "# This is because the default loss function and optimization algorithms assume that the outputs are real-valued (continuous).\n",
        "\n",
        "# The to_categorical() function from Keras' utils module does just that: it converts integer-encoded categorical labels into one-hot \n",
        "# encoded arrays.\n",
        "#\n",
        "# Here's what happens:\n",
        "#    For each sample in y_train and y_test, you get a vector of length num_classes, where:\n",
        "#       The index corresponding to the true class label is set to 1 (i.e., True or 1).@\n",
        "#       All other indices are set to 0 (i.e., False or 0).\n",
        "# This one-hot encoded representation allows you to use categorical cross-entropy loss functions, such as categorical_crossentropy, \n",
        "# which is commonly used for multi-class classification problems.\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Now, \\'y_train\\' labels has been \\'hot-encoded\\' and look like below : the correpsonding label is where you see a \\'1\\'.</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>This is necessary to use the \\'categorical cross-entropy loss function\\'</h1>\"))\n",
        "\n",
        "# Loop through each sample in the training data and print its label again (after conversion)\n",
        "for i in range(10):\n",
        "  print(y_train[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uwQcpaBuUTe_"
      },
      "source": [
        "# Prepare Data for the training phase\n",
        "\n",
        "### Step 1 : Normalize input images in theh range [0,1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "yN6M-is5T0nu"
      },
      "outputs": [],
      "source": [
        "# Normalize the pixel values of the training images by dividing each value by 255.0\n",
        "\n",
        "# This code is normalizing the pixel values of the training and test images to be between 0 and 1, which is a common practice in deep learning models.\n",
        "# In this case, it's likely that the original pixel values were between 0 and 255 (i.e., 8-bit grayscale), so dividing each value by 255.0 brings \n",
        "# them down to the range [0, 1]. This normalization helps to:\n",
        "#   Reduce the effect of large pixel values on the model's training and testing\n",
        "#   Make the model more robust to small variations in brightness or contrast\n",
        "#   Improve the convergence and accuracy of the model\n",
        "# Note that this is a simple form of data normalization. \n",
        "# In some cases, you may need to use more sophisticated techniques, such as mean subtraction or standardization, depending on the specific characteristics \n",
        "# of your dataset.\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0\n",
        "\n",
        "# Pick a random image from the x_train set with a random index\n",
        "random_idx = np.random.choice(len(x_train))\n",
        "image = x_train[random_idx].reshape((28, 28))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>We picked a random image at index \\'{random_idx}\\' in the x_train MNIST dataset.</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The image matrix from \\'x_train\\'x transformed with decimal values .xxxx is displayed below.</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The corresponding \\'y_train\\'x encoded label is:  \\'{y_train[random_idx]}\\'</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>===> Can you say which number is it ??</h1>\"))\n",
        "\n",
        "#print(\"\\nRandom image at index\", random_idx, \":\")\n",
        "#print(image) \n",
        "\n",
        "# Print the image matrix with values in the range [0,1]\n",
        "for row in image:\n",
        "    for pixel in row:\n",
        "        if pixel > 127:\n",
        "            print(\"{:.4f}\".format(pixel), end=' ')  # Display pixel value with 3 decimal places\n",
        "        else:\n",
        "            print(\"{:.4f}\".format(pixel), end=' ')  # Display pixel value with 3 decimal places\n",
        "    print()\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>...It is a {np.where(y_train[random_idx] ==1)[0][0]} !</h1>\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prepare Data for the training phase\n",
        "\n",
        "### Step 2 : Reshape images from  (height, width, channels) to (samples, features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "colab_type": "code",
        "id": "x2kNxq2rUsoG",
        "outputId": "ddf2b84e-130c-42ac-a720-1b638d270c1d"
      },
      "outputs": [],
      "source": [
        "# Reshape the test images from (height, width, channels) to (samples, features)\n",
        "\n",
        "# The -1 in the reshape method is a special value that tells NumPy to automatically calculate the size based on the other dimensions. \n",
        "# In this case, it means that the first dimension (the number of samples) will remain unchanged, and the second dimension (the number of features) \n",
        "# will be calculated as the product of the height, width, and channels.\n",
        "\n",
        "# Before reshaping, x_train likely had a shape like ( Samples, Height, Width, Channels ), where each image is represented by a 4D array with three \n",
        "# dimensions for spatial information (height, width, and channels).\n",
        "\n",
        "# After reshaping, x_train will have a shape like (Samples, Features), where the number of features is equal to the product of height, width, and channels. \n",
        "# This is often referred to as a \"flat\" or \"vectorized\" representation, which is suitable for inputting into neural networks.\n",
        "\n",
        "# By printing the shape of x_train using print(x_train.shape), you can verify that the reshaping was successful and check the new shape of your data.\n",
        "\n",
        "x_train = x_train.reshape(x_train.shape[0], -1)\n",
        "x_test = x_test.reshape(x_test.shape[0], -1)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>We reshaped the input \\'x_train\\' images to be flat with the 28x28 pixels of each images.</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>\\'x_train\\' shape is now: {x_train.shape} so we have 60000 images of 784 pixels each</h1>\"))\n",
        "\n",
        "#print(\"\\nx_train shape:\", x_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9-K1km4JVQpe"
      },
      "source": [
        "# Create the Model : a Fully Fast Forward Connected Neural Network (CNN\n",
        "- Step 1: Create the input flaten layer (768 pixels = 28 x 28)\n",
        "\n",
        "- Step 2 : Create one or several hidden layers (adjustable depths with variable '_hidden_layer_depth')\n",
        "\n",
        "- Step 3 : Create the output layer : here we have 10 modalities (or classes), so 10 output neurons for the classifier\n",
        "\n",
        "- Step 4 : print the NN layout and total numbers of parameters (it can easyly reach 1 Millions parametres !!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "colab_type": "code",
        "id": "pOE4hD2HUxZt",
        "outputId": "0748afca-9037-427d-80d0-3b0ba8902ee1"
      },
      "outputs": [],
      "source": [
        "# Create a neural network model using the Sequential API\n",
        "model = Sequential()\n",
        "\n",
        "# INPUT LAYER\n",
        "\n",
        "# Add a dense (fully-connected) layer with default of '_hidden_layer_depth'units, input shape (784,), and ReLU activation\n",
        "# This layer will process the input data and produce an output\n",
        "model.add(Dense(units=_hidden_layer_depth, input_shape=(784,), activation='relu'))\n",
        "\n",
        "# HIDDEN LAYERS\n",
        "\n",
        "for _ in range(_hidden_layer_number):\n",
        "    # Add another dense layer with a default of 128 units and ReLU activation\n",
        "    # This layer will process the output from the previous layer and produce another output\n",
        "    model.add(Dense(units=_hidden_layer_depth, activation='relu'))\n",
        "\n",
        "# DROPUT LAYER\n",
        "\n",
        "# Add a dropout layer with a drop rate of 0.25 (25%)\n",
        "# Dropout randomly sets 25% of the neurons to zero during training to prevent overfitting\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# OUTPUT LAYER\n",
        "\n",
        "# Add a final dense layer with 10 units and softmax activation\n",
        "# This layer will produce a probability distribution over the 10 classes\n",
        "model.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# Compile the model with categorical cross-entropy loss function, Adam optimizer, and accuracy metric\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Our Neural network is composed of {_hidden_layer_number} hidden layers with each {_hidden_layer_depth} neurons:</h1>\"))\n",
        "\n",
        "# Print a summary of the model architecture using the `summary()` method\n",
        "model.summary()\n",
        "\n",
        "#                     +---------------+\n",
        "#                     |  Input Layer  |\n",
        "#                     +---------------+\n",
        "#                             |\n",
        "#                             | (784,)\n",
        "#                             v\n",
        "# +-----------------------+       +-----------------------+\n",
        "# |      Dense Layer      |       |      Dense Layer      |\n",
        "# |      (128, ReLU)      |       |      (128, ReLU)      |\n",
        "# +-----------------------+       +-----------------------+\n",
        "#                             |\n",
        "#                             | (0.25 dropout)\n",
        "#                             v\n",
        "# +-----------------------+       +-----------------------+\n",
        "# |      Dropout Layer    |       |        None           |\n",
        "# +-----------------------+       +-----------------------+\n",
        "#                             |\n",
        "#                             |\n",
        "#                             v\n",
        "# +-----------------------+       +-----------------------+\n",
        "# |     Final Dense Layer |       |  Softmax Activation   |\n",
        "# |      (10, Softmax)    |       |                       |\n",
        "# +-----------------------+       +-----------------------+\n",
        "\n",
        "# Note that this schema is a simplified representation of the model architecture. \n",
        "# The actual connections between layers may be more complex and involve additional operations such as convolutional layers, pooling layers, etc.\n",
        "\n",
        "#print()\n",
        "#plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7bk3ylbU1Ws4"
      },
      "source": [
        "# Train the model by using tmethod 'fit' and define a bath_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "colab_type": "code",
        "id": "d75pU9l31S52",
        "outputId": "6ebaa4c8-fd10-4ce0-8773-56f0e1e2f621"
      },
      "outputs": [],
      "source": [
        "# Define the batch size for training (number of samples to process in parallel)\n",
        "#batch_size = 512\n",
        "\n",
        "# Define the number of epochs for training (iterations over the entire dataset)\n",
        "#epochs=10\n",
        "\n",
        "# Full stack trace mode for debugging\n",
        "#keras.config.disable_traceback_filtering()\n",
        "\n",
        "# Train the model using the `fit` method\n",
        "# x_train: input data, y_train: target labels\n",
        "\n",
        "#print()\n",
        "\n",
        "# Start the timer\n",
        "start_time = time.time()\n",
        "\n",
        "model.fit(x=x_train, y=y_train, \n",
        "          validation_data=(x_test, y_test), \n",
        "          callbacks=[PlotLossesKeras()],\n",
        "          verbose = 0,\n",
        "          batch_size=_training_batch_size,\n",
        "          epochs=_training_epochs)\n",
        "\n",
        "# End the timer\n",
        "end_time = time.time()\n",
        "\n",
        "# Calculate the elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>The model.fit() function to train our model took {elapsed_time:.2f} seconds to run.</h1>\"))\n",
        "#print(f\"The model.fit function took {elapsed_time:.2f} seconds to run.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J9Wa4rvE1qMs"
      },
      "source": [
        "# Evaluate the model by calculationg the Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "colab_type": "code",
        "id": "_4lONmsI1g9y",
        "outputId": "7a33457e-21c6-4299-c1af-f14743f5cff6"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test data using the `evaluate` method\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Loss after training phase = {round(test_loss, 2)}</h1>\"))\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Accuracy after training phase = {round(test_acc, 2)}</h1>\"))\n",
        "\n",
        "\n",
        "#print(\"\\nTest Loss: {}, Test Accuracy: {}\".format(test_loss, test_acc))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Use the model to make predictions on the x_test dataset (never seen by the model during training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "colab_type": "code",
        "id": "wi7yXzCI10Kr",
        "outputId": "213eccb4-ad5c-4764-f2af-f589cad1a059"
      },
      "outputs": [],
      "source": [
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Now we can predict all labels for the \\'y_test\\' testing set:</h1>\"))\n",
        "\n",
        "# Here's what each part does:\n",
        "#    model.predict(x_test) : This line uses the trained model to make predictions on the test data. \n",
        "#    The output is a NumPy array containing the predicted probability distribution for each sample in the test data.\n",
        "#\n",
        "#    y_pred_classes = np.argmax(y_pred, axis=1) : This line converts the predicted probabilities into class labels by taking the index of \n",
        "#    the maximum value along the specified axis (in this case, axis 1). The output is a NumPy array containing the predicted class labels for each sample in the test data.\n",
        "#\n",
        "#    print(y_pred) : This line prints out the predicted probability distributions for each sample in the test data. \n",
        "#    Each row represents a single prediction, and the columns represent the different classes (or outcomes) that the model can predict.\n",
        "#\n",
        "#    print(y_pred_classes) : This line prints out the predicted class labels for each sample in the test data.\n",
        "\n",
        "# Use the model to make predictions on the test data\n",
        "y_pred = model.predict(x_test)\n",
        "\n",
        "# Convert the predicted probabilities into class labels using `argmax`\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "#print(y_pred)\n",
        "#print(y_pred_classes)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>y_pred_classes table contains all predicted labels (with a total of {len(y_pred_classes)} labels)</h1>\"))\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>y_pred_classes table contains all predicted labels : {y_pred_classes}</h1>\"))\n",
        "\n",
        "# Dataframe with predicted labels to have a better display...\n",
        "df = pd.DataFrame(y_pred_classes, columns=['Predicted Classes'])\n",
        "df = df.reset_index(drop=True).reset_index().rename(columns={'index': 'y_pred Index'})\n",
        "\n",
        "# Display 100 fisrt predicted labels\n",
        "print(df.head(20).to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pick a random image form x_test the run the prediction and display results with 'predict' and 'true'\n",
        "\n",
        "Note : each run of this cell will pick a different image form the x_test dataset (random index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "colab_type": "code",
        "id": "UxaVbpGo2ZOm",
        "outputId": "fa584997-bf0b-4524-bfa9-26242a01d4c6"
      },
      "outputs": [],
      "source": [
        "# Single PREDICTION Example on a random sample from the x_test dataset (not known during the training phase)\n",
        "\n",
        "# Select a single random example from the test data for visualization by choosing a random index within the length of x_test\n",
        "random_idx = np.random.choice(len(x_test))\n",
        "#print(x_test[random_idx].shape)\n",
        "#print(x_test[random_idx])\n",
        "\n",
        "# Get the input data (features) at the chosen index\n",
        "x_sample = x_test[random_idx]\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Predicting Image at Index = {random_idx}, Size of x_sample = {len(x_sample)}</h1>\"))\n",
        "#print(f'\\nPredicting Image at Index = {random_idx}, Size of x_sample = {len(x_sample)}')\n",
        "\n",
        "# Now, extract the true and predicted class labels for this example\n",
        "\n",
        "# And now, we have to onvert the one-hot encoded label to a class label (0-9)\n",
        "\n",
        "# Let's explain more in depth this part:\n",
        "#   np.argmax: This is a function from the NumPy library that finds the index of the maximum value along a specified axis.\n",
        "#   y_test: This is a 2D NumPy array representing the true labels for the test data. Each row corresponds to a single sample, and each column represents a possible class label (in this case, digit classes 0-9).\n",
        "#   axis=1: This specifies that we want to find the maximum value along the second axis (i.e., the rows). In other words, we're looking for the most likely class label for each individual sample.\n",
        "# When you run this code, NumPy will iterate through each row of y_test, find the maximum value in that row, and return the corresponding index. \n",
        "# => This is equivalent to converting the one-hot encoded labels into a single class label (e.g., [0, 0, 1, 0] becomes 2, since 2 is the most likely class).\n",
        "# The resulting array y_true will contain the true class labels for each sample in the test data.\n",
        "y_true = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Get the true class label for this example\n",
        "y_sample_true = y_true[random_idx]\n",
        "\n",
        "# Get the predicted class label for this example\n",
        "y_sample_pred_class = y_pred_classes[random_idx]\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>\\'Predicted Label\\' versus \\'True label\\' in y_test</h1>\"))\n",
        "\n",
        "plt.title(\"Predicted: {}, True: {}\".format(y_sample_pred_class, y_sample_true), fontsize=16)\n",
        "plt.imshow(x_sample.reshape(28, 28), cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q2kYQNGJ3aN5"
      },
      "source": [
        "# Calculate and display the Confusion Matrix of the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 621
        },
        "colab_type": "code",
        "id": "77SFgjZ53KJ4",
        "outputId": "dde148b9-f8dd-4b54-cf6b-464c0e32c835"
      },
      "outputs": [],
      "source": [
        "# This code calculates the confusion matrix : a table that summarizes the predictions made by a classification model, using the true and predicted labels. \n",
        "# Then, it creates a plot for the confusion matrix using Seaborn's heatmap function. \n",
        "# The annot=True argument means that each cell in the heatmap will be annotated with the number of correct/incorrect predictions. \n",
        "# The fmt='d' argument specifies the format of the annotations (in this case, decimal integers).\n",
        "\n",
        "# Calculate the confusion matrix using true and predicted labels\n",
        "confusion_mtx = confusion_matrix(y_true, y_pred_classes)\n",
        "\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display the Confusion Matrix of our model showing \\'Predicted\\' versus \\'True\\'</h1>\"))\n",
        "\n",
        "# Create a plot for the confusion matrix\n",
        "fig, ax = plt.subplots(figsize=(15,10))\n",
        "ax = sns.heatmap(confusion_mtx, annot=True, fmt='d', ax=ax, cmap=\"Blues\")\n",
        "ax.set_xlabel('Predicted Label')\n",
        "ax.set_ylabel('True Label')\n",
        "ax.set_title('Confusion Matrix');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n-9WrS6w4TO0"
      },
      "source": [
        "# Investigate Errors of our Model, in which case it fails do predict correctly ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "colab_type": "code",
        "id": "99OZQyuI5H6h",
        "outputId": "95258142-08a0-42c4-db42-280d86f844fa"
      },
      "outputs": [],
      "source": [
        "# This code identifies the incorrect predictions by subtracting the true labels from the predicted class labels. \n",
        "# It then extracts the relevant information (predicted classes, probabilities, true labels, and input data) only for these incorrect predictions.\n",
        "\n",
        "# The errors array is a boolean mask that indicates which indices in the original arrays correspond to incorrect predictions.\n",
        "# The indexing operation [errors] uses this mask to extract the corresponding elements from each array, resulting in separate arrays \n",
        "# for the errors (incorrect predictions).\n",
        "\n",
        "# Create a boolean array indicating where predictions are incorrect\n",
        "errors = (y_pred_classes - y_true != 0)\n",
        "\n",
        "# Extract the predicted classes, predicted probabilities, true labels, and input data for incorrect predictions only\n",
        "y_pred_classes_errors = y_pred_classes[errors]  # Predicted class labels for errors\n",
        "y_pred_errors = y_pred[errors]                  # Predicted probabilities for errors\n",
        "y_true_errors = y_true[errors]                  # True class labels for errors\n",
        "x_test_errors = x_test[errors]                  # Input data (features) for errors\n",
        "\n",
        "# Calculate the maximum predicted error probability for each sample\n",
        "y_pred_errors_probability = np.max(y_pred_errors, axis=1)\n",
        "\n",
        "# Extract the true class probability errors from the predicted errors\n",
        "true_probability_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n",
        "\n",
        "# Compute the difference between the predicted error probability and the true class probability error\n",
        "diff_errors_pred_true = y_pred_errors_probability - true_probability_errors\n",
        "\n",
        "# Get list of indices of sorted differences\n",
        "sorted_idx_diff_errors = np.argsort(diff_errors_pred_true)\n",
        "top_idx_diff_errors = sorted_idx_diff_errors[-5:] # 5 last ones\n",
        "\n",
        "# Show Top Errors\n",
        "num = len(top_idx_diff_errors)\n",
        "f, ax = plt.subplots(1, num, figsize=(30,30))\n",
        "\n",
        "print()\n",
        "display(HTML(f\"<h1 style='color:{_output_cell_color};'>Display samples where our model is failing the most (and compare with the confusion matrix above)</h1>\"))\n",
        "\n",
        "\n",
        "for i in range(0, num):\n",
        "  idx = top_idx_diff_errors[i]\n",
        "  sample = x_test_errors[idx].reshape(28,28)\n",
        "  y_t = y_true_errors[idx]\n",
        "  y_p = y_pred_classes_errors[idx]\n",
        "  ax[i].imshow(sample, cmap='gray')\n",
        "  ax[i].set_title(\"Predicted label :{}\\nTrue label: {}\".format(y_p, y_t), fontsize=22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Congratulations you reached the dnd of the simple Computer Vision Demo ! \n",
        "\n",
        "## Summary of what you did :\n",
        "\n",
        "### - Import the clean MNIST dataset from keras\n",
        "### - Split the dataset with a training set and a testing set\n",
        "### - Visualize the dataset \n",
        "### - Prepare the dataset for the training phase\n",
        "### - Build a simple Neural Network model from scratch\n",
        "### - Train the model\n",
        "### - Evaluate the model (accuracy and confusion matrix)\n",
        "### - Investigate errors of the model\n",
        "\n",
        "# Thanks !"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "MNIST Digits Classfication.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
